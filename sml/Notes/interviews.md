# 过拟合
## L1/L2 正则化
正则化是w权重的值偏小，趋势每层的输出值偏小。对于激活函数来说类似于把输出聚集在0附近， 而在0附近激活函数类似线性，这就降低了激活函数的非线性功能

原因：通过正则化，使神经元的输出偏小，类似于消除了部分神经元，使得网络变得简单。存在一个状态使得模型比较好的拟合输入数据。
## dropout 也是正则化
注意scale，作用：使神经元的期望和不使用dropout一致
## data augumentation
## early stop
同时优化两个问题
- 减小loss
- 防止过拟合

问题：提前结果会导致loss比较大
# Relu
优点包括：
- 解决了梯度消失、爆炸的问题
- 计算方便，计算速度快，求导方便
- 加速网络训练

缺点包括：
- 由于负数部分恒为0，会导致一些神经元无法激活
- 输出不是以0为中心
# softmax计算中的问题
$$
a_i = \frac{\exp(a_i)}{\sum_{j=0}^N{\exp(a_j)}}
$$
当$a_i$过大时，可能会导致$\exp(a_i)$溢出，使用时可以归一化计算
$$
\exp(a_i) = \frac{\exp(a_i)}{\exp(\max(a_j))}=\exp(a_i-\max(a_j)) \leq 1
$$
# 梯度消失与爆炸
第一个问题相对简单，由于反向传播过程中，前面网络权重的偏导数的计算是逐渐从后往前累乘的，如果使用sigmoid，tanh激活函数的话，由于导数小于一，因此累乘会逐渐变小，导致梯度消失，前面的网络层权重更新变慢；

如果权重w本身比较大，累乘会导致前面网络的参数偏导数变大，产生数值上溢。因为 sigmoid 导数最大为1/4，故只有当abs(w)>4时才可能出现梯度爆炸，因此最普遍发生的是梯度消失问题。

解决方法：
- 使用ReLU等激活函数，梯度只会为0或者1，每层的网络都可以得到相同的更新速度
- 采用LSTM进行梯度裁剪(clip), 如果梯度值大于某个阈值，我们就进行梯度裁剪，限制在一个范围内
- 使用正则化，这样会限制参数w的大小，从而防止梯度爆炸
- 设计网络层数更少的网络
- 进行模型训练batch normalization

## 激活函数
### sigmoid
$$
f(x) = \frac{1}{1+e^{-x}}\\
f'(x) = -\frac{e^{-x}}{(1+e^{-x})^2} \leq -\frac{1}{4}
$$
### tanh
$$
f(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}\\
f'(x) = \frac{2e^{2x}}{(e^{2x}+1)^2} \leq \frac{1}{2}
$$

## 非平衡数据集的处理方法有哪些？
- 采用更好的评价指标，例如F1、AUC曲线等，而不是Recall、Precision
- 进行过采样，随机重复少类别的样本来增加它的数量；
- 进行欠采样，随机对多类别样本降采样
- 通过在已有数据上添加噪声来生成新的数据
- 修改损失函数，添加新的惩罚项，使得小样本的类别被判断错误的损失增大，迫使模型重视小样本的数据
- 使用组合/集成方法解决样本不均衡，在每次生成训练集时使用所有分类中的小样本量，同时从分类中的大样本量中随机抽取数据来与小样本量合并构成训练集，这样反复多次会得到很多训练集和训练模型。最后在应用时，使用组合方法（例如投票、加权投票等）产生分类预测结果；
## CRF与HMM模型的区别？
- 大的不同点是linear-CRF模型是判别模型，而HMM是生成模型，即linear-CRF模型要优化求解的是条件概率P(y|x),则HMM要求解的是联合分布P(x,y)。
- 第二，linear-CRF是利用最大熵模型的思路去建立条件概率模型，对于观测序列并没有做马尔科夫假设。而HMM是在对观测序列做了马尔科夫假设的前提下建立联合分布的模型。
## 交叉熵与kl散度
$$
\begin{aligned}
    KL(p||q) &= -\int p(x)\ln(q(x)) {\rm d}x - \int p(x)\ln(p(x)) {\rm d}x \\
    &= CrossEntropy(p(x)) - \int p(x)\ln(p(x)){\rm d}x\\
    const &= -\int p(x)\ln(p(x)){\rm d}x\\
\end{aligned}
$$
优化交叉熵等同于优化相对熵，即KL散度