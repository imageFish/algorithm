## 过拟合
### L1/L2 正则化
正则化是w权重的值偏小，趋势每层的输出值偏小。对于激活函数来说类似于把输出聚集在0附近， 而在0附近激活函数类似线性，这就降低了激活函数的非线性功能

原因：通过正则化，使神经元的输出偏小，类似于消除了部分神经元，使得网络变得简单。存在一个状态使得模型比较好的拟合输入数据。
### dropout 也是正则化
注意scale，作用：使神经元的期望和不适用dropout一致
### data augumentation
### early stop
同时优化两个问题
- 减小loss
- 防止过拟合

问题：提前结果会导致loss比较大