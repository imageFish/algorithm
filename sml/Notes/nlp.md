
## 语言模型中，Bert为什么在masked language model中采用了80%、10%、10%的策略？
答：如果训练的时候100%都是Mask，那么在fine-tune的时候，所有的词时候已知的，不存在[Mask]，那么模型就只知道根据其他词的信息来预测当前词，而不会直接利用这个词本身的信息，会凭空损失一部分信息，对下游任务不利。

还有 10% random token 是因为如果都用原 token，模型在预训练时可能会偷懒，不去建模单词间的依赖关系，直接照抄当前词

[MASK] 是以一种显式的方式告诉模型『这个词我不告诉你，你自己从上下文里猜』，从而防止信息泄露。如果 [MASK] 以外的部分全部都用原 token，模型会学到『如果当前词是 [MASK]，就根据其他词的信息推断这个词；如果当前词是一个正常的单词，就直接抄输入』。这样一来，在 finetune 阶段，所有词都是正常单词，模型就照抄所有词，不提取单词间的依赖关系了。

以一定的概率填入 random token，就是让模型时刻堤防着，在任意 token 的位置都需要把当前 token 的信息和上下文推断出的信息相结合。这样一来，在 finetune 阶段的正常句子上，模型也会同时提取这两方面的信息，因为它不知道它所看到的『正常单词』到底有没有被动过手脚的。

    注：为了fine-tune阶段，使用了在mask的概率上使用原来词和替换为随机词，替换为mask三种概率操作，这样让模型去分辨当前词和上下文之间的关系。如果只有mask，那么模型可能会只依赖上下文，对fine-tune不利。加入随机词和原来词，让模型学会分辨当前词和上下文之间的关系。
## Bert现有的问题有哪些？

答：Bert模型过于庞大，参数太多，无论是feature-based approach还是fine-tune approach都很慢；

而且因为表示是上下文相关的，上线的应用需要实时处理，时间消耗太大；Bert给出来的中文模型中，是以字为基本单位的，很多需要词向量的应用无法直接使用；同时该模型无法识别很多生僻词，都是UNK；

Bert模型作为自回归模型，由于模型结构的问题，无法给出句子概率值

    注：自回归模型和生成模型之间的关系。（待补）

## 一些小问题
### 1. word2vec和tf-idf 相似度计算时的区别？
word2vec 1、稠密的 低维度的 2、表达出相似度； 3、表达能力强；4、泛化能力强；

注：
- tfidf计算相似度可能需要将两篇文档中的所有词都考虑，一一对应。导致维度很高
### 2. word2vec和NNLM对比有什么区别
- 其本质都可以看作是语言模型；
- 词向量只不过NNLM一个产物，word2vec虽然其本质也是语言模型，但是其专注于词向量本身，因此做了许多优化来提高计算效率：
- 与NNLM相比，词向量直接sum，不再拼接，并舍弃隐层；
- 考虑到sofmax归一化需要遍历整个词汇表，采用hierarchical softmax 和negative sampling进行优化，hierarchical softmax 实质上生成一颗带权路径最小的哈夫曼树，让高频词搜索路劲变小；negative sampling更为直接，实质上对每一个样本中每一个词都进行负例采样；
### 3. word2vec负采样有什么作用？
负采样这个点引入word2vec非常巧妙，两个作用，1.加速了模型计算，2.保证了模型训练的效果，一个是模型每次只需要更新采样的词的权重，不用更新所有的权重，那样会很慢，第二，中心词其实只跟它周围的词有关系，位置离着很远的词没有关系，也没必要同时训练更新，作者这点非常聪明。